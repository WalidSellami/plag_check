

تميل الخصوصية إلى المناقشة في سياق خصوصية البيانات وحماية البيانات وأمن البيانات، وقد سمحت هذه المخاوف لصانعي السياسات باتخاذ المزيد من الخطوات هنا في السنوات الأخيرة. على سبيل المثال، في عام 2016، تم إنشاء تشريع الناتج المحلي الإجمالي لحماية البيانات الشخصية للأشخاص في الاتحاد الأوروبي والمنطقة الاقتصادية الأوروبية، مما يمنح الأفراد مزيدًا من التحكم في بياناتهم. في الولايات المتحدة، تعمل الولايات الفردية على تطوير سياسات، مثل قانون خصوصية المستهلك في كاليفورنيا (CCPA)، والذي يتطلب من الشركات إبلاغ المستهلكين حول جمع بياناتهم. لقد أجبر هذا التشريع الأخير الشركات على إعادة التفكير في كيفية تخزين واستخدام بيانات التعريف الشخصية (PII). ونتيجة لذلك، أصبحت الاستثمارات في مجال الأمن أولوية متزايدة بالنسبة للشركات في سعيها إلى القضاء على أي نقاط ضعف وفرص للمراقبة والقرصنة والهجمات الإلكترونية.
التحيز والتمييز

أثارت حالات التحيز والتمييز عبر عدد من الأنظمة الذكية العديد من الأسئلة الأخلاقية فيما يتعلق باستخدام الذكاء الاصطناعي. كيف يمكننا الحماية من التحيز والتمييز عندما تكون مجموعات بيانات التدريب عرضة للتحيز؟ في حين أن الشركات عادةً ما يكون لديها نوايا حسنة النية فيما يتعلق بجهود الأتمتة الخاصة بها، إلا أنه قد تكون هناك عواقب غير متوقعة لدمج الذكاء الاصطناعي في ممارسات التوظيف. في إطار جهودها لأتمتة وتبسيط العملية، قامت أمازون ، عن غير قصد، بتحيز المرشحين المحتملين للوظائف حسب الجنس للأدوار الفنية المفتوحة، واضطرت في النهاية إلى إلغاء المشروع. مع ظهور مثل هذه الأحداث، أثارت Harvard Business Review (الرابط موجود خارج موقع ibm.com) أسئلة محددة أخرى حول استخدام الذكاء الاصطناعي في ممارسات التوظيف، مثل ما هي البيانات التي يجب أن تكون قادرًا على استخدامها عند تقييم مرشح لمنصب ما.

لا يقتصر التحيز والتمييز على وظيفة الموارد البشرية أيضًا؛ ويمكن العثور عليها في عدد من التطبيقات، بدءًا من برامج التعرف على الوجه وحتى خوارزميات الوسائط الاجتماعية.

نظرًا لأن الشركات أصبحت أكثر وعيًا بمخاطر الذكاء الاصطناعي، فقد أصبحت أيضًا أكثر نشاطًا في هذه المناقشة حول أخلاقيات وقيم الذكاء الاصطناعي. على سبيل المثال، صرح أرفيند كريشنا، الرئيس التنفيذي لشركة IBM في العام الماضي، أن شركة IBM قد أوقفت منتجاتها للتعرف على الوجه وتحليله ذات الأغراض العامة، مؤكدًا أن "IBM تعارض بشدة ولن تتغاضى عن استخدامات أي تقنية، بما في ذلك تقنية التعرف على الوجه التي يقدمها بائعون آخرون، للأغراض الجماعية". المراقبة، والتنميط العنصري، وانتهاكات حقوق الإنسان والحريات الأساسية، أو أي غرض لا يتوافق مع قيمنا ومبادئ الثقة والشفافية.
مسئولية

لا يوجد تشريع عالمي وشامل ينظم ممارسات الذكاء الاصطناعي، لكن العديد من البلدان والدول تعمل على تطويرها وتنفيذها محليًا. وقد أصبحت بعض القواعد التنظيمية الخاصة بالذكاء الاصطناعي موجودة اليوم، ومن المنتظر أن يكون هناك المزيد منها في المستقبل. ولسد هذه الفجوة، ظهرت الأطر الأخلاقية كجزء من التعاون بين علماء الأخلاق والباحثين للتحكم في بناء نماذج الذكاء الاصطناعي وتوزيعها داخل المجتمع. ومع ذلك، في الوقت الحالي، لا تعمل هذه إلا على التوجيه، ويظهر البحث (الرابط موجود خارج موقع ibm.com) أن الجمع بين المسؤولية الموزعة والافتقار إلى البصيرة بشأن العواقب المحتملة لا يؤدي بالضرورة إلى منع الضرر بالمجتمع.
